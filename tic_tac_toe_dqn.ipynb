{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Tic-Toc-Toe-environment\" data-toc-modified-id=\"Tic-Toc-Toe-environment-1\">Tic Toc Toe environment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialization-and-attributes\" data-toc-modified-id=\"Initialization-and-attributes-1.1\">Initialization and attributes</a></span></li><li><span><a href=\"#Taking-actions\" data-toc-modified-id=\"Taking-actions-1.2\">Taking actions</a></span></li><li><span><a href=\"#Reward\" data-toc-modified-id=\"Reward-1.3\">Reward</a></span></li></ul></li><li><span><a href=\"#Optimal-policy-for-Tic-Toc-Toe-environment\" data-toc-modified-id=\"Optimal-policy-for-Tic-Toc-Toe-environment-2\">Optimal policy for Tic Toc Toe environment</a></span><ul class=\"toc-item\"><li><span><a href=\"#An-example-of-optimal-player-playing-against-random-player\" data-toc-modified-id=\"An-example-of-optimal-player-playing-against-random-player-2.1\">An example of optimal player playing against random player</a></span></li><li><span><a href=\"#An-example-of-optimal-player-playing-against-optimal-player\" data-toc-modified-id=\"An-example-of-optimal-player-playing-against-optimal-player-2.2\">An example of optimal player playing against optimal player</a></span></li></ul></li><li><span><a href=\"#Performance-measures\" data-toc-modified-id=\"Performance-measures-3\">Performance measures</a></span></li><li><span><a href=\"#2-TicTacToe-with-DQN\" data-toc-modified-id=\"2-TicTacToe-with-DQN-4\">2 TicTacToe with DQN</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.0-Implementation-details\" data-toc-modified-id=\"2.0-Implementation-details-4.1\">2.0 Implementation details</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-4.1.1\">Setup</a></span></li><li><span><a href=\"#Utility\" data-toc-modified-id=\"Utility-4.1.2\">Utility</a></span></li><li><span><a href=\"#Replay-memory\" data-toc-modified-id=\"Replay-memory-4.1.3\">Replay memory</a></span></li><li><span><a href=\"#Illegal-moves-reward-(TODO)\" data-toc-modified-id=\"Illegal-moves-reward-(TODO)-4.1.4\">Illegal moves reward (TODO)</a></span></li><li><span><a href=\"#DQN-algorithm\" data-toc-modified-id=\"DQN-algorithm-4.1.5\">DQN algorithm</a></span></li></ul></li><li><span><a href=\"#2.1-Learning-from-experts\" data-toc-modified-id=\"2.1-Learning-from-experts-4.2\">2.1 Learning from experts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Q11-Standard-training-with-fixed--$\\epsilon$\" data-toc-modified-id=\"Q11-Standard-training-with-fixed--$\\epsilon$-4.2.1\">Q11 Standard training with fixed  $\\epsilon$</a></span></li><li><span><a href=\"#Q12-Training-without-the-replay-buffer-and-with-a-batch-size-of-1\" data-toc-modified-id=\"Q12-Training-without-the-replay-buffer-and-with-a-batch-size-of-1-4.2.2\">Q12 Training without the replay buffer and with a batch size of 1</a></span></li><li><span><a href=\"#Q13-Training-with-decreasing-$\\epsilon$-given-different-values-of-$n*$\" data-toc-modified-id=\"Q13-Training-with-decreasing-$\\epsilon$-given-different-values-of-$n*$-4.2.3\">Q13 Training with decreasing $\\epsilon$ given different values of $n*$</a></span></li><li><span><a href=\"#Q14-Visualizing-$M_{opt}$-and-$M_{rand}$-over-time\" data-toc-modified-id=\"Q14-Visualizing-$M_{opt}$-and-$M_{rand}$-over-time-4.2.4\">Q14 Visualizing $M_{opt}$ and $M_{rand}$ over time</a></span></li><li><span><a href=\"#Q15-Reporting-best-results\" data-toc-modified-id=\"Q15-Reporting-best-results-4.2.5\">Q15 Reporting best results</a></span></li></ul></li><li><span><a href=\"#2.2-Learning-by-self-practice\" data-toc-modified-id=\"2.2-Learning-by-self-practice-4.3\">2.2 Learning by self-practice</a></span><ul class=\"toc-item\"><li><span><a href=\"#Q16-Training-with-different-fixed--$\\epsilon$\" data-toc-modified-id=\"Q16-Training-with-different-fixed--$\\epsilon$-4.3.1\">Q16 Training with different fixed  $\\epsilon$</a></span></li><li><span><a href=\"#Q17-Training-with-decreasing--$\\epsilon$-given-different-values-of-$n*$\" data-toc-modified-id=\"Q17-Training-with-decreasing--$\\epsilon$-given-different-values-of-$n*$-4.3.2\">Q17 Training with decreasing  $\\epsilon$ given different values of $n*$</a></span></li><li><span><a href=\"#Q18-Reporting-best-results\" data-toc-modified-id=\"Q18-Reporting-best-results-4.3.3\">Q18 Reporting best results</a></span></li><li><span><a href=\"#Q19-Visualizing-Q-values\" data-toc-modified-id=\"Q19-Visualizing-Q-values-4.3.4\">Q19 Visualizing Q values</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# imported\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# customized\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 1st game is the famous Tic Toc Toe. You can read about the game and its rules here: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "We implemented the game as an environment in the style of games in the [Python GYM library](https://gym.openai.com/). The commented source code is available in the file \"tic_env.py\". Here, we give a brief introduction to the environment and how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize the environment / game as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which then has the following attributes with the corresponding initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grid': array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " 'end': False,\n",
       " 'winner': None,\n",
       " 'player2value': {'X': 1, 'O': -1},\n",
       " 'num_step': 0,\n",
       " 'current_player': 'X'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game is played by two players: player 'X' and player 'O'. The attribute 'current_player' shows whose turn it is. We assume that player 'X' always plays first.\n",
    "\n",
    "The attribute 'grid' is a 3x3 numpy array and presents the board in the real game and the state $s_t$ in the reinfocement learning language. Each elements can take a value in {0, 1, -1}:\n",
    "     0 : place unmarked\n",
    "     1 : place marked with X \n",
    "    -1 : place marked with O \n",
    "        \n",
    "The attribute 'end' shows if the game is over or not, and the attribute 'winner' shows the winner of the game: either \"X\", \"O\", or None.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use function 'render' to visualize the current position of the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|- - -|\n",
      "|- - -|\n",
      "|- - -|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment will recieve action from two players in turn and update the grid. At each time, one player can take the action $a_t$, where $a_t$ can either be an integer between 0 to 8 or a touple, corresponding to the 9 possible.\n",
    "\n",
    "Function 'step' is used to recieve the action of the player, update the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " False,\n",
       " None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|- - X|\n",
      "|- - -|\n",
      "|- - -|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grid': array([[0., 0., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " 'end': False,\n",
       " 'winner': None,\n",
       " 'player2value': {'X': 1, 'O': -1},\n",
       " 'num_step': 1,\n",
       " 'current_player': 'O'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  1.],\n",
       "        [ 0., -1.,  0.],\n",
       "        [ 0.,  0.,  0.]]),\n",
       " False,\n",
       " None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|- - X|\n",
      "|- O -|\n",
      "|- - -|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grid': array([[ 0.,  0.,  1.],\n",
       "        [ 0., -1.,  0.],\n",
       "        [ 0.,  0.,  0.]]),\n",
       " 'end': False,\n",
       " 'winner': None,\n",
       " 'player2value': {'X': 1, 'O': -1},\n",
       " 'num_step': 2,\n",
       " 'current_player': 'X'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all actions are available at each time: One cannot choose a place which has been taken before. There is an error if an unavailable action is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# env.step((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward is always 0 until the end of the game. When the game is over, the reward is 1 if you win the game, -1 if you lose, and 0 besides. Function 'observe' can be used after each step to recieve the new state $s_t$, whether the game is over, and the winner, and function 'reward' to get the reward value $r_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  1.],\n",
       "        [ 0., -1.,  0.],\n",
       "        [ 0.,  0.,  0.]]),\n",
       " False,\n",
       " None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of finishing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.,  1.,  1.],\n",
       "        [-1., -1.,  0.],\n",
       "        [ 0.,  0.,  0.]]),\n",
       " True,\n",
       " 'X')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|X X X|\n",
      "|O O -|\n",
      "|- - -|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.,  1.,  1.],\n",
       "        [-1., -1.,  0.],\n",
       "        [ 0.,  0.,  0.]]),\n",
       " True,\n",
       " 'X')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy for Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know the exact optimal policy for Tic Toc Toe. We have implemented and $\\epsilon$-greedy version of optimal polciy which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player = OptimalPlayer(epsilon = 0., player = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_player.act(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_player.player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of optimal player playing against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player = O\n",
      "Random player = X\n",
      "|X O X|\n",
      "|X O X|\n",
      "|O X O|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player X\n",
      "Optimal player = X\n",
      "Random player = O\n",
      "|O - X|\n",
      "|- X -|\n",
      "|X - O|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player X\n",
      "Optimal player = X\n",
      "Random player = O\n",
      "|X O -|\n",
      "|X X -|\n",
      "|X O O|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player = X\n",
      "Random player = O\n",
      "|X O O|\n",
      "|O X X|\n",
      "|X X O|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player O\n",
      "Optimal player = O\n",
      "Random player = X\n",
      "|O X X|\n",
      "|- O X|\n",
      "|- - O|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(grid)\n",
    "        else:\n",
    "            move = player_rnd.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player = ' +  Turns[0])\n",
    "            print('Random player = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of optimal player playing against optimal player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = X\n",
      "Optimal player 2 = O\n",
      "|X O X|\n",
      "|X O O|\n",
      "|O X X|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = O\n",
      "Optimal player 2 = X\n",
      "|X X O|\n",
      "|O O X|\n",
      "|X O X|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = X\n",
      "Optimal player 2 = O\n",
      "|X O X|\n",
      "|O O X|\n",
      "|X X O|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = O\n",
      "Optimal player 2 = X\n",
      "|O X X|\n",
      "|X O O|\n",
      "|X O X|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = O\n",
      "Optimal player 2 = X\n",
      "|X O X|\n",
      "|O O X|\n",
      "|X X O|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            move = player_opt_2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  Turns[0])\n",
    "            print('Optimal player 2 = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(agent1, agent2, env, switch: bool):\n",
    "    grid, end, __  = env.observe()\n",
    "    if switch:\n",
    "        agent1.player, agent2.player = 'O', 'X'\n",
    "    else:\n",
    "        agent1.player, agent2.player = 'X', 'O'\n",
    "    while end == False:\n",
    "        if env.current_player == agent1.player:\n",
    "            move = agent1.act(grid) \n",
    "            grid, end, winner = env.step(move, print_grid=False) \n",
    "        else:\n",
    "            move = agent2.act(grid)\n",
    "            grid, end, winner = env.step(move, print_grid=False) \n",
    "    return winner, agent1, agent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(agent1, agent2, n_episode: int = 500) -> Dict:\n",
    "    env = TictactoeEnv()\n",
    "    win, los, draw = 0, 0, 0\n",
    "    res = []\n",
    "        \n",
    "    for episode in tqdm(range(n_episode)):\n",
    "        env.reset()\n",
    "        switch = i % 2\n",
    "        winner, agent1, agent2 = run_episode(agent1, agent2, env, switch)\n",
    "\n",
    "        if winner == agent1.player:\n",
    "            win += 1\n",
    "            res.append(1)\n",
    "        elif winner == agent2.player:\n",
    "            los += 1\n",
    "            res.append(-1)\n",
    "        else:\n",
    "            draw += 1\n",
    "            res.append(0)\n",
    "    \n",
    "    res_info = {\n",
    "        'win': win,\n",
    "        'los': los,\n",
    "        'draw': draw,\n",
    "        'res': res,\n",
    "        'metric': (win-los)/n_episode, \n",
    "        'draw_rate': draw/n_episode\n",
    "    }\n",
    "           \n",
    "    return res_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:02<00:00, 221.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Eval with Opt(0.0)\n",
      "Mopt = 0.0, Draw rate = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 466.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Eval with Opt(1.0)\n",
      "Mrand = 0.964, Draw rate = 0.036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metric_dict = {'opt': 0.0, 'rand': 1.0}\n",
    "for (mode, epsilon) in metric_dict.items():\n",
    "    player_opt = OptimalPlayer(epsilon=0.)\n",
    "    player_baseline = OptimalPlayer(epsilon=epsilon)\n",
    "    res_info = eval(player_opt, player_baseline)\n",
    "    \n",
    "    print(\"# Eval with Opt({})\".format(epsilon))\n",
    "    print('M{} = {}, Draw rate = {}'.format(mode, res_info['metric'], res_info['draw_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def res_plot(epi_res: List, window: int = 250) -> None:\n",
    "    results = np.mean(np.array(epi_res).reshape(-1,window), axis=1)\n",
    "    fig, axes = plt.subplots(figsize=(6, 5))\n",
    "    axes.plot(results)\n",
    "    axes.set_title('Result with avg. window = {} (Totol = {})'.format(window, len(epi_res)))\n",
    "    \n",
    "epi_res = res_info['res']\n",
    "res_plot(epi_res, window=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 TicTacToe with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std\n",
    "import io\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# imported\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a feedforward neural net with three hidden layers consisting of 128, 256, and 128 neurons\n",
    "- input: 9 poition x 2 possible states\n",
    "- output: nine actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "GAMMA = 0.99 # discount factor\n",
    "TARGET_UPDATE = 500 # update interval\n",
    "START_LR = 5e-4\n",
    "EPS_MAX = 0.8\n",
    "EPS_MIN = 0.1\n",
    "N_STAR = 20000\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_to_state(\n",
    "    grid: np.array, \n",
    "    switch: bool = False, \n",
    "    vec: bool = True,\n",
    ") -> np.array:\n",
    "    \"\"\"If not switched, 1 and -1 on the grid means X and O, and vice versa\"\"\"\n",
    "    \n",
    "    state = np.zeros((3, 3, 2))\n",
    "    \n",
    "    if not switch:\n",
    "        state[:, :, 0] = (grid==1).astype(float)\n",
    "        state[:, :, 1] = (grid==-1).astype(float)\n",
    "    else:\n",
    "        state[:, :, 0] = (grid==-1).astype(float)\n",
    "        state[:, :, 1] = (grid==1).astype(float)\n",
    "    \n",
    "    if vec:\n",
    "        state = state.reshape(1,-1)\n",
    "    \n",
    "    return torch.tensor(state, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.]\n",
      " [-1.  0.  1.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "grid, _, _ = env.reset()\n",
    "grid, _, _ = env.step(2)\n",
    "grid, _, _ = env.step(3)\n",
    "grid, _, _ = env.step(5)\n",
    "print(grid)\n",
    "test_state1 = grid_to_state(grid, vec=False)\n",
    "# print(test_state1[:,:,0], test_state1[:,:,1])\n",
    "test_state2 = grid_to_state(grid, switch=True, vec=False)\n",
    "# print(test_state2[:,:,0], test_state2[:,:,1])\n",
    "assert np.array_equal(test_state1[:,:,0], test_state2[:,:,1])\n",
    "assert np.array_equal(test_state2[:,:,0], test_state1[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_state2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False, False,  True, False,  True,  True,  True])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_state3 = grid_to_state(grid, switch=True, vec=True)\n",
    "valid_moves = (test_state3.cpu().numpy().reshape(3,3,2).sum(axis=2).reshape(-1) == 0)\n",
    "valid_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Transition`: a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the board position (showing -, O, or X)\n",
    "- `ReplayMemory`: a cyclic buffer of bounded size that holds the transitions observed recently, which could be sampled for selecting a random batch of transitions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#replay-memory\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def last(self):\n",
    "        return self.memory[-1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example of memory replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Transition(state=1, action=0, next_state=2, reward=0.35329627648252715), Transition(state=5, action=1, next_state=6, reward=0.984568235161103)] [Transition(state=9, action=1, next_state=None, reward=0.9905004637349177)]\n",
      "Batch:  Transition(state=(1, 5), action=(0, 1), next_state=(2, 6), reward=(0.35329627648252715, 0.984568235161103)) Transition(state=(9,), action=(1,), next_state=(None,), reward=(0.9905004637349177,))\n"
     ]
    }
   ],
   "source": [
    "memory = ReplayMemory(100)\n",
    "for i in range(10):\n",
    "    action = random.randrange(2)\n",
    "    reward = random.random()\n",
    "    if i==9:\n",
    "        memory.push(i, action, None, reward)\n",
    "    else:\n",
    "        memory.push(i, action, i+1, reward)\n",
    "transitions1 = memory.sample(2)\n",
    "transitions2 = [memory.last()]\n",
    "print(transitions1, transitions2)\n",
    "assert type(transitions1) == type(transitions2)\n",
    "\n",
    "# unzip as batch\n",
    "batch1 = Transition(*zip(*transitions1))\n",
    "batch2 = Transition(*zip(*transitions2))\n",
    "print(\"Batch: \", batch1, batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_final_next_states = [s for s in batch2.next_state if s is not None]\n",
    "non_final_next_states\n",
    "# if None, report []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illegal moves reward (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_UNAV = -1\n",
    "# invalid = env.check_valid(position) == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input state $s_t$ by a 3 × 3 × 2 tensor\n",
    "- 2 hidden layers each with 128 neurons – with ReLu activation functions.\n",
    "The output layer has 9 neurons (for 9 different actions) with linear activation functions\n",
    "- 9 neurons (for 9 different actions) with linear activation functions\n",
    "\n",
    "PS: do not constraint actions to only available actions. However, whenever the agent takes an unavailable action, we end the game and give the agent a negative reward\n",
    "of value $r_{unav}$ = −1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs: int = 18, n_outputs: int = 9):\n",
    "        super(DQN, self).__init__()\n",
    "        self.i2h = nn.Linear(n_inputs, 128)\n",
    "        self.hid = nn.Linear(128, 128)\n",
    "        self.h2o = nn.Linear(128, n_outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization.\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.i2h(x))\n",
    "        x = F.relu(self.hid(x))\n",
    "        x = self.h2o(x)\n",
    "        return x\n",
    "    \n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(state).max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_exploration(\n",
    "    n_step: int,\n",
    "    n_star: int = 20000, \n",
    "    e_max: float = 0.8, \n",
    "    e_min: float = 0.1, \n",
    "):\n",
    "    return max(e_min, e_max * (1 - n_step/n_star)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(\n",
    "    eps: float,\n",
    "    policy_net: nn.Module, \n",
    "    state: np.array, \n",
    "    device: torch.device, \n",
    "    nr_action: int = 9,\n",
    ") -> Tuple[torch.tensor, bool]:\n",
    "\n",
    "    if random.random() > eps:\n",
    "        return policy_net.act(state), False\n",
    "    else:\n",
    "        return (\n",
    "            torch.tensor(\n",
    "                [[random.randrange(9)]],\n",
    "                device=device,\n",
    "            ),\n",
    "            True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(\n",
    "    batch_size: int,\n",
    "    gamma: float,\n",
    "    device: torch.device,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: torch.nn,\n",
    "    policy: nn.Module,\n",
    "    target: nn.Module,\n",
    "    replay: bool = True,\n",
    "    memory: ReplayMemory = None,\n",
    "):\n",
    "    \"\"\"Model optimization step, borrow from the Torch DQN tutorial.\n",
    "    \n",
    "    Arguments:\n",
    "        batch_size {int} -- Number of observations to use per batch step\n",
    "        gamma {float} -- Reward discount factor\n",
    "        device {torch.device} -- Device\n",
    "        optimizer {torch.optim.Optimizer} -- Optimizer\n",
    "        criterion {torch.nn} -- Loss\n",
    "        policy {nn.Module} -- Policy net\n",
    "        target {nn.Module} -- Target net\n",
    "        replay {bool} -- Use replay buffer for not\n",
    "        memory {ReplayMemory} -- Replay memory\n",
    "    \"\"\"\n",
    "    if replay:\n",
    "        if len(memory) < batch_size:\n",
    "            return\n",
    "        transitions = memory.sample(batch_size)\n",
    "        # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "        # detailed explanation). This converts batch-array of Transitions\n",
    "        # to Transition of batch-arrays.\n",
    "    else:\n",
    "        # update the network by using only the latest transition\n",
    "        transitions = [memory.last()]\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        device=device,\n",
    "        dtype=torch.bool,\n",
    "    )\n",
    "    non_final_next_states_ = [s for s in batch.next_state if s is not None]\n",
    "    # TODO: check correct or not with batch_size = 1\n",
    "    if len(non_final_next_states_) == 0:\n",
    "        return\n",
    "    non_final_next_states = torch.cat(non_final_next_states_)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy\n",
    "    state_action_values = policy(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    next_state_values[non_final_mask] = target(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = criterion(\n",
    "        state_action_values, \n",
    "        expected_state_action_values.unsqueeze(1)\n",
    "    )\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()\n",
    "    \n",
    "    # return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    n_episode: int = 20000,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    buffer_size: int = BUFFER_SIZE,\n",
    "    gamma: float = GAMMA,\n",
    "    target_update: int = TARGET_UPDATE,\n",
    "    explore: bool = False,\n",
    "    replay: bool = True,\n",
    "    n_star: int = N_STAR, \n",
    "    e_max: float = EPS_MAX, \n",
    "    e_min: float = EPS_MIN, \n",
    "    lr: float = START_LR,\n",
    "    logging_size = 2000,\n",
    "    seed = None,\n",
    "    device: torch.device = None, \n",
    "    save_ckpt: str = 'policy.pth',\n",
    ") -> List:\n",
    "\n",
    "    logging.info(\"Beginning training on: {}\".format(device))\n",
    "    \n",
    "    # Network\n",
    "    policy = DQN(n_inputs = 18, n_outputs = 9).to(device)\n",
    "    target = DQN(n_inputs = 18, n_outputs = 9).to(device)\n",
    "    target.load_state_dict(policy.state_dict())\n",
    "    target.eval()\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    \n",
    "    # TODO: scheduler?\n",
    "\n",
    "    # Huber loss (delte=1 in SmoothL1Loss)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    # Memory buffer\n",
    "    if replay:\n",
    "        memory = ReplayMemory(BUFFER_SIZE)\n",
    "    else:\n",
    "        memory = ReplayMemory(9)\n",
    "\n",
    "    env = TictactoeEnv()\n",
    "    # state = torch.tensor([], dtype=torch.float).to(device)\n",
    "    info = {\n",
    "        \"total\": 0,\n",
    "        \"illegals\": 0,\n",
    "        \"tie_g\": 0,\n",
    "        \"win_g\": 0,\n",
    "        \"los_g\": 0,\n",
    "        \"win_rate\": 0.0,\n",
    "        \"eps\": 0.0,\n",
    "        \"loss\": []\n",
    "    }\n",
    "    summaries = []\n",
    "    \n",
    "    # Expert\n",
    "    expert = OptimalPlayer(epsilon=.5)\n",
    "\n",
    "    for episode in tqdm(range(n_episode)):\n",
    "        switch = episode % 2\n",
    "        if switch:\n",
    "            player1, player2 = 'O', 'X'\n",
    "        else:\n",
    "            player1, player2 = 'X', 'O'\n",
    "        expert.player = player2\n",
    "        \n",
    "        grid, end, _ = env.reset()\n",
    "        \n",
    "        if explore:\n",
    "            eps = decreasing_exploration(episode, n_star, e_max, e_min)\n",
    "        else:\n",
    "            eps = e_max\n",
    "        \n",
    "        # Take first step by expert\n",
    "        if env.num_step == 0 and switch:\n",
    "            move = expert.act(grid)\n",
    "            grid, end, winner = env.step(move, print_grid=False) \n",
    "            \n",
    "        while not end:\n",
    "            # Select and perform an action from DQN\n",
    "            state = grid_to_state(grid, switch=switch, vec=True).to(device)\n",
    "            action, _ = select_action(eps, policy, state, device)\n",
    "            if env.check_valid(action.item()):\n",
    "                grid, end, winner = env.step(action.item(), print_grid=False)\n",
    "                reward = env.reward(player=player1)\n",
    "            else:\n",
    "                reward = R_UNAV # -1\n",
    "                end = True\n",
    "                info[\"illegals\"] += 1\n",
    "                winner = player2\n",
    "                # print(episode, \"Illegal moves\")\n",
    "                \n",
    "            if not end:\n",
    "                move = expert.act(grid)\n",
    "                grid, end, winner = env.step(move, print_grid=False) \n",
    "                # Observe new state\n",
    "                next_state = grid_to_state(grid, switch=switch, vec=True).to(device)\n",
    "            else:\n",
    "                next_state = None\n",
    "            \n",
    "            memory.push(state, action, next_state, torch.tensor([reward], device=device))\n",
    "\n",
    "            optimize_model(\n",
    "                batch_size=batch_size,\n",
    "                gamma=gamma,\n",
    "                device=device,\n",
    "                optimizer=optimizer,\n",
    "                criterion=criterion,\n",
    "                policy=policy,\n",
    "                target=target,\n",
    "                replay=replay,\n",
    "                memory=memory,\n",
    "            )\n",
    "            \n",
    "        # update summary\n",
    "        info['total'] = episode + 1\n",
    "        if winner == None:\n",
    "            info['tie_g'] += 1\n",
    "        elif winner == player1:\n",
    "            info['win_g'] += 1\n",
    "        else:\n",
    "            info['los_g'] += 1\n",
    "        info['win_rate'] = info['win_g'] / info['total']\n",
    "        info['eps'] = eps\n",
    "        # info['loss'] = loss\n",
    "        summaries.append(info)\n",
    "        \n",
    "        if episode and ((episode+1) % target_update) == 0:\n",
    "            target.load_state_dict(policy.state_dict())\n",
    "        if episode and ((episode+1) % logging_size) == 0:\n",
    "            # print(info)\n",
    "            [print(key,':',value) for key, value in info.items()]\n",
    "\n",
    "    logging.info(\"Complete\")\n",
    "    torch.save(policy.state_dict(), 'policy.pth')\n",
    "    logging.info(\"Save policy as {}!\".format(save_ckpt))\n",
    "    \n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Beginning training on: cuda\n",
      " 25%|██▌       | 2518/10000 [00:17<00:56, 132.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total : 2501\n",
      "illegals : 2150\n",
      "tie_g : 0\n",
      "win_g : 32\n",
      "los_g : 2469\n",
      "win_rate : 0.012794882047181128\n",
      "eps : 0.2\n",
      "loss : []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5022/10000 [00:32<00:27, 183.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total : 5001\n",
      "illegals : 4333\n",
      "tie_g : 0\n",
      "win_g : 59\n",
      "los_g : 4942\n",
      "win_rate : 0.011797640471905619\n",
      "eps : 0.2\n",
      "loss : []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 7543/10000 [00:45<00:11, 221.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total : 7501\n",
      "illegals : 6647\n",
      "tie_g : 0\n",
      "win_g : 65\n",
      "los_g : 7436\n",
      "win_rate : 0.008665511265164644\n",
      "eps : 0.2\n",
      "loss : []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:58<00:00, 169.86it/s]\n",
      "INFO:root:Complete\n",
      "INFO:root:Policy saved!\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "summary = fit(\n",
    "    n_episode=10000, \n",
    "    target_update=500, \n",
    "    logging_size=2500, \n",
    "    explore=False, \n",
    "    e_max=0.2,\n",
    "    n_star=10000,\n",
    "    device=DEVICE,\n",
    "    replay=False,\n",
    "    batch_size=1, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Beginning training on: cuda\n",
      " 25%|██▌       | 2510/10000 [00:29<01:55, 64.82it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total : 2500\n",
      "illegals : 513\n",
      "tie_g : 55\n",
      "win_g : 756\n",
      "los_g : 1689\n",
      "win_rate : 0.3024\n",
      "eps : 0.2\n",
      "loss : []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5016/10000 [00:59<00:50, 97.85it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total : 5000\n",
      "illegals : 952\n",
      "tie_g : 86\n",
      "win_g : 1541\n",
      "los_g : 3373\n",
      "win_rate : 0.3082\n",
      "eps : 0.2\n",
      "loss : []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 7514/10000 [01:26<00:26, 93.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total : 7500\n",
      "illegals : 1453\n",
      "tie_g : 106\n",
      "win_g : 2255\n",
      "los_g : 5139\n",
      "win_rate : 0.3006666666666667\n",
      "eps : 0.2\n",
      "loss : []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [01:53<00:00, 87.92it/s]\n",
      "INFO:root:Complete\n",
      "INFO:root:Save policy as policy.pth!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total : 10000\n",
      "illegals : 1920\n",
      "tie_g : 149\n",
      "win_g : 2967\n",
      "los_g : 6884\n",
      "win_rate : 0.2967\n",
      "eps : 0.2\n",
      "loss : []\n"
     ]
    }
   ],
   "source": [
    "summary = fit(\n",
    "    n_episode=10000, \n",
    "    target_update=500, \n",
    "    logging_size=2500, \n",
    "    explore=False, \n",
    "    e_max=0.2,\n",
    "    n_star=10000,\n",
    "    device=DEVICE,\n",
    "    replay=True,\n",
    "    batch_size=64, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_history = [info['loss'] for info in summary]\n",
    "# res_plot(loss_history, window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNPlayer():\n",
    "    def __init__(self, device: torch.device, player='X'):\n",
    "        self.player = player # 'X' or 'O'\n",
    "        self.device = device\n",
    "        self.model = DQN(n_inputs=18, n_outputs=9).to(self.device)\n",
    "        \n",
    "    def load_model(self, path: str):\n",
    "        model_state_dict = torch.load(path, map_location=self.device)\n",
    "        self.model.load_state_dict(model_state_dict)\n",
    "        self.model.eval()\n",
    "\n",
    "    def act(self, grid: np.array):\n",
    "        if self.player == 'X':\n",
    "            switch = False\n",
    "        else:\n",
    "            switch = True\n",
    "        state = grid_to_state(grid, switch=switch, vec=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            pred = F.softmax(self.model.forward(state), dim=0).cpu().numpy()\n",
    "            valid_moves = (state.cpu().numpy().reshape(3,3,2).sum(axis=2).reshape(-1) == 0)\n",
    "            move = (valid_moves * pred).argmax()\n",
    "            return int(move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "i2h.weight \t torch.Size([128, 18])\n",
      "i2h.bias \t torch.Size([128])\n",
      "hid.weight \t torch.Size([128, 128])\n",
      "hid.bias \t torch.Size([128])\n",
      "h2o.weight \t torch.Size([9, 128])\n",
      "h2o.bias \t torch.Size([9])\n"
     ]
    }
   ],
   "source": [
    "player_dqn = DQNPlayer(device=DEVICE)\n",
    "player_dqn.load_model('policy.pth')\n",
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in player_dqn.model.state_dict():\n",
    "    print(param_tensor, \"\\t\", player_dqn.model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 284.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Eval with Opt(0.0)\n",
      "Mopt = -1.0, Draw rate = 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 508.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Eval with Opt(1.0)\n",
      "Mrand = -0.066, Draw rate = 0.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metric_dict = {'opt': 0.0, 'rand': 1.0}\n",
    "for (mode, epsilon) in metric_dict.items():\n",
    "    # player_opt = OptimalPlayer(epsilon=0.)\n",
    "    player_baseline = OptimalPlayer(epsilon=epsilon)\n",
    "    res_info = eval(player_dqn, player_baseline)\n",
    "    \n",
    "    print(\"# Eval with Opt({})\".format(epsilon))\n",
    "    print('M{} = {}, Draw rate = {}'.format(mode, res_info['metric'], res_info['draw_rate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Learning from experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11 Standard training with fixed  $\\epsilon$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12 Training without the replay buffer and with a batch size of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q13 Training with decreasing $\\epsilon$ given different values of $n*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q14 Visualizing $M_{opt}$ and $M_{rand}$ over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q15 Reporting best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Learning by self-practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q16 Training with different fixed  $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q17 Training with decreasing  $\\epsilon$ given different values of $n*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q18 Reporting best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q19 Visualizing Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.792px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
