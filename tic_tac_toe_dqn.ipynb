{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Tic-Toc-Toe-environment\" data-toc-modified-id=\"Tic-Toc-Toe-environment-1\">Tic Toc Toe environment</a></span><ul class=\"toc-item\"><li><span><a href=\"#Initialization-and-attributes\" data-toc-modified-id=\"Initialization-and-attributes-1.1\">Initialization and attributes</a></span></li><li><span><a href=\"#Taking-actions\" data-toc-modified-id=\"Taking-actions-1.2\">Taking actions</a></span></li><li><span><a href=\"#Reward\" data-toc-modified-id=\"Reward-1.3\">Reward</a></span></li></ul></li><li><span><a href=\"#Optimal-policy-for-Tic-Toc-Toe-environment\" data-toc-modified-id=\"Optimal-policy-for-Tic-Toc-Toe-environment-2\">Optimal policy for Tic Toc Toe environment</a></span><ul class=\"toc-item\"><li><span><a href=\"#An-example-of-optimal-player-playing-against-random-player\" data-toc-modified-id=\"An-example-of-optimal-player-playing-against-random-player-2.1\">An example of optimal player playing against random player</a></span></li><li><span><a href=\"#An-example-of-optimal-player-playing-against-optimal-player\" data-toc-modified-id=\"An-example-of-optimal-player-playing-against-optimal-player-2.2\">An example of optimal player playing against optimal player</a></span></li></ul></li><li><span><a href=\"#Performance-measures\" data-toc-modified-id=\"Performance-measures-3\">Performance measures</a></span></li><li><span><a href=\"#2-TicTacToe-with-DQN\" data-toc-modified-id=\"2-TicTacToe-with-DQN-4\">2 TicTacToe with DQN</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.0-Implementation-details\" data-toc-modified-id=\"2.0-Implementation-details-4.1\">2.0 Implementation details</a></span><ul class=\"toc-item\"><li><span><a href=\"#Setup\" data-toc-modified-id=\"Setup-4.1.1\">Setup</a></span></li><li><span><a href=\"#Utility\" data-toc-modified-id=\"Utility-4.1.2\">Utility</a></span></li><li><span><a href=\"#Replay-memory\" data-toc-modified-id=\"Replay-memory-4.1.3\">Replay memory</a></span></li><li><span><a href=\"#Illegal-moves-reward-(TODO)\" data-toc-modified-id=\"Illegal-moves-reward-(TODO)-4.1.4\">Illegal moves reward (TODO)</a></span></li><li><span><a href=\"#DQN-algorithm\" data-toc-modified-id=\"DQN-algorithm-4.1.5\">DQN algorithm</a></span></li></ul></li><li><span><a href=\"#2.1-Learning-from-experts\" data-toc-modified-id=\"2.1-Learning-from-experts-4.2\">2.1 Learning from experts</a></span><ul class=\"toc-item\"><li><span><a href=\"#Q11-Standard-training-with-fixed--$\\epsilon$\" data-toc-modified-id=\"Q11-Standard-training-with-fixed--$\\epsilon$-4.2.1\">Q11 Standard training with fixed  $\\epsilon$</a></span></li><li><span><a href=\"#Q12-Training-without-the-replay-buffer-and-with-a-batch-size-of-1\" data-toc-modified-id=\"Q12-Training-without-the-replay-buffer-and-with-a-batch-size-of-1-4.2.2\">Q12 Training without the replay buffer and with a batch size of 1</a></span></li><li><span><a href=\"#Q13-Training-with-decreasing-$\\epsilon$-given-different-values-of-$n*$\" data-toc-modified-id=\"Q13-Training-with-decreasing-$\\epsilon$-given-different-values-of-$n*$-4.2.3\">Q13 Training with decreasing $\\epsilon$ given different values of $n*$</a></span></li><li><span><a href=\"#Q14-Visualizing-$M_{opt}$-and-$M_{rand}$-over-time\" data-toc-modified-id=\"Q14-Visualizing-$M_{opt}$-and-$M_{rand}$-over-time-4.2.4\">Q14 Visualizing $M_{opt}$ and $M_{rand}$ over time</a></span></li><li><span><a href=\"#Q15-Reporting-best-results\" data-toc-modified-id=\"Q15-Reporting-best-results-4.2.5\">Q15 Reporting best results</a></span></li></ul></li><li><span><a href=\"#2.2-Learning-by-self-practice\" data-toc-modified-id=\"2.2-Learning-by-self-practice-4.3\">2.2 Learning by self-practice</a></span><ul class=\"toc-item\"><li><span><a href=\"#Q16-Training-with-different-fixed--$\\epsilon$\" data-toc-modified-id=\"Q16-Training-with-different-fixed--$\\epsilon$-4.3.1\">Q16 Training with different fixed  $\\epsilon$</a></span></li><li><span><a href=\"#Q17-Training-with-decreasing--$\\epsilon$-given-different-values-of-$n*$\" data-toc-modified-id=\"Q17-Training-with-decreasing--$\\epsilon$-given-different-values-of-$n*$-4.3.2\">Q17 Training with decreasing  $\\epsilon$ given different values of $n*$</a></span></li><li><span><a href=\"#Q18-Reporting-best-results\" data-toc-modified-id=\"Q18-Reporting-best-results-4.3.3\">Q18 Reporting best results</a></span></li><li><span><a href=\"#Q19-Visualizing-Q-values\" data-toc-modified-id=\"Q19-Visualizing-Q-values-4.3.4\">Q19 Visualizing Q values</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# imported\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# customized\n",
    "from tic_env import TictactoeEnv, OptimalPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our 1st game is the famous Tic Toc Toe. You can read about the game and its rules here: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
    "\n",
    "We implemented the game as an environment in the style of games in the [Python GYM library](https://gym.openai.com/). The commented source code is available in the file \"tic_env.py\". Here, we give a brief introduction to the environment and how it can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization and attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can initialize the environment / game as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TictactoeEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which then has the following attributes with the corresponding initial values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grid': array([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " 'end': False,\n",
       " 'winner': None,\n",
       " 'player2value': {'X': 1, 'O': -1},\n",
       " 'num_step': 0,\n",
       " 'current_player': 'X'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game is played by two players: player 'X' and player 'O'. The attribute 'current_player' shows whose turn it is. We assume that player 'X' always plays first.\n",
    "\n",
    "The attribute 'grid' is a 3x3 numpy array and presents the board in the real game and the state $s_t$ in the reinfocement learning language. Each elements can take a value in {0, 1, -1}:\n",
    "     0 : place unmarked\n",
    "     1 : place marked with X \n",
    "    -1 : place marked with O \n",
    "        \n",
    "The attribute 'end' shows if the game is over or not, and the attribute 'winner' shows the winner of the game: either \"X\", \"O\", or None.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use function 'render' to visualize the current position of the board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|- - -|\n",
      "|- - -|\n",
      "|- - -|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game environment will recieve action from two players in turn and update the grid. At each time, one player can take the action $a_t$, where $a_t$ can either be an integer between 0 to 8 or a touple, corresponding to the 9 possible.\n",
    "\n",
    "Function 'step' is used to recieve the action of the player, update the grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " False,\n",
       " None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|- - X|\n",
      "|- - -|\n",
      "|- - -|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grid': array([[0., 0., 1.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]]),\n",
       " 'end': False,\n",
       " 'winner': None,\n",
       " 'player2value': {'X': 1, 'O': -1},\n",
       " 'num_step': 1,\n",
       " 'current_player': 'O'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  1.],\n",
       "        [ 0., -1.,  0.],\n",
       "        [ 0.,  0.,  0.]]),\n",
       " False,\n",
       " None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|- - X|\n",
      "|- O -|\n",
      "|- - -|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'grid': array([[ 0.,  0.,  1.],\n",
       "        [ 0., -1.,  0.],\n",
       "        [ 0.,  0.,  0.]]),\n",
       " 'end': False,\n",
       " 'winner': None,\n",
       " 'player2value': {'X': 1, 'O': -1},\n",
       " 'num_step': 2,\n",
       " 'current_player': 'X'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But not all actions are available at each time: One cannot choose a place which has been taken before. There is an error if an unavailable action is taken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# env.step((0,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reward is always 0 until the end of the game. When the game is over, the reward is 1 if you win the game, -1 if you lose, and 0 besides. Function 'observe' can be used after each step to recieve the new state $s_t$, whether the game is over, and the winner, and function 'reward' to get the reward value $r_t$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  1.],\n",
       "        [ 0., -1.,  0.],\n",
       "        [ 0.,  0.,  0.]]),\n",
       " False,\n",
       " None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of finishing the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.,  1.,  1.],\n",
       "        [-1., -1.,  0.],\n",
       "        [ 0.,  0.,  0.]]),\n",
       " True,\n",
       " 'X')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)\n",
    "env.step(3)\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|X X X|\n",
      "|O O -|\n",
      "|- - -|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.,  1.,  1.],\n",
       "        [-1., -1.,  0.],\n",
       "        [ 0.,  0.,  0.]]),\n",
       " True,\n",
       " 'X')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "env.reward(player='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reward(player='O')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal policy for Tic Toc Toe environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we know the exact optimal policy for Tic Toc Toe. We have implemented and $\\epsilon$-greedy version of optimal polciy which you can use for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_player = OptimalPlayer(epsilon = 0., player = 'X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_player.act(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'X'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_player.player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of optimal player playing against random player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Game end, winner is player O\n",
      "Optimal player = O\n",
      "Random player = X\n",
      "|X O X|\n",
      "|X O O|\n",
      "|- O X|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player O\n",
      "Optimal player = O\n",
      "Random player = X\n",
      "|X - X|\n",
      "|X O X|\n",
      "|O O O|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player X\n",
      "Optimal player = X\n",
      "Random player = O\n",
      "|X - -|\n",
      "|- X O|\n",
      "|O - X|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player X\n",
      "Optimal player = X\n",
      "Random player = O\n",
      "|X - -|\n",
      "|O X -|\n",
      "|- O X|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player O\n",
      "Optimal player = O\n",
      "Random player = X\n",
      "|- X O|\n",
      "|X O -|\n",
      "|O X -|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_rnd = OptimalPlayer(epsilon=1., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt.act(grid)\n",
    "        else:\n",
    "            move = player_rnd.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player = ' +  Turns[0])\n",
    "            print('Random player = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of optimal player playing against optimal player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = X\n",
      "Optimal player 2 = O\n",
      "|O X O|\n",
      "|X X O|\n",
      "|X O X|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = O\n",
      "Optimal player 2 = X\n",
      "|X X O|\n",
      "|O O X|\n",
      "|X O X|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = X\n",
      "Optimal player 2 = O\n",
      "|O X X|\n",
      "|X O O|\n",
      "|X O X|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = X\n",
      "Optimal player 2 = O\n",
      "|X O X|\n",
      "|O O X|\n",
      "|X X O|\n",
      "\n",
      "-------------------------------------------\n",
      "Game end, winner is player None\n",
      "Optimal player 1 = O\n",
      "Optimal player 2 = X\n",
      "|X O X|\n",
      "|O O X|\n",
      "|X X O|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Turns = np.array(['X','O'])\n",
    "for i in range(5):\n",
    "    env.reset()\n",
    "    grid, _, __ = env.observe()\n",
    "    Turns = Turns[np.random.permutation(2)]\n",
    "    player_opt_1 = OptimalPlayer(epsilon=0., player=Turns[0])\n",
    "    player_opt_2 = OptimalPlayer(epsilon=0., player=Turns[1])\n",
    "    for j in range(9):\n",
    "        if env.current_player == player_opt.player:\n",
    "            move = player_opt_1.act(grid)\n",
    "        else:\n",
    "            move = player_opt_2.act(grid)\n",
    "\n",
    "        grid, end, winner = env.step(move, print_grid=False)\n",
    "\n",
    "        if end:\n",
    "            print('-------------------------------------------')\n",
    "            print('Game end, winner is player ' + str(winner))\n",
    "            print('Optimal player 1 = ' +  Turns[0])\n",
    "            print('Optimal player 2 = ' +  Turns[1])\n",
    "            env.render()\n",
    "            env.reset()\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(agent1, agent2, env, switch: bool):\n",
    "    grid, end, __  = env.observe()\n",
    "    if switch:\n",
    "        agent1.player, agent2.player = 'O', 'X'\n",
    "    else:\n",
    "        agent1.player, agent2.player = 'X', 'O'\n",
    "    while end == False:\n",
    "        if env.current_player == agent1.player:\n",
    "            move = agent1.act(grid) \n",
    "            grid, end, winner = env.step(move, print_grid=False) \n",
    "        else:\n",
    "            move = agent2.act(grid)\n",
    "            grid, end, winner = env.step(move, print_grid=False) \n",
    "    return winner, agent1, agent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(agent1, agent2, n_episode: int = 500) -> Dict:\n",
    "    env = TictactoeEnv()\n",
    "    win, los, draw = 0, 0, 0\n",
    "    res = []\n",
    "        \n",
    "    for episode in tqdm(range(n_episode)):\n",
    "        env.reset()\n",
    "        switch = i % 2\n",
    "        winner, agent1, agent2 = run_episode(agent1, agent2, env, switch)\n",
    "\n",
    "        if winner == agent1.player:\n",
    "            win += 1\n",
    "            res.append(1)\n",
    "        elif winner == agent2.player:\n",
    "            los += 1\n",
    "            res.append(-1)\n",
    "        else:\n",
    "            draw += 1\n",
    "            res.append(0)\n",
    "    \n",
    "    res_info = {\n",
    "        'win': win,\n",
    "        'los': los,\n",
    "        'draw': draw,\n",
    "        'res': res,\n",
    "        'metric': (win-los)/n_episode, \n",
    "        'draw_rate': draw/n_episode\n",
    "    }\n",
    "           \n",
    "    return res_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:02<00:00, 217.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Eval with Opt(0.0)\n",
      "Mopt = 0.0, Draw rate = 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:01<00:00, 444.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Eval with Opt(1.0)\n",
      "Mrand = 0.964, Draw rate = 0.036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "metric_dict = {'opt': 0.0, 'rand': 1.0}\n",
    "for (mode, epsilon) in metric_dict.items():\n",
    "    player_opt = OptimalPlayer(epsilon=0.)\n",
    "    player_baseline = OptimalPlayer(epsilon=epsilon)\n",
    "    res_info = eval(player_opt, player_baseline)\n",
    "    \n",
    "    print(\"# Eval with Opt({})\".format(epsilon))\n",
    "    print('M{} = {}, Draw rate = {}'.format(mode, res_info['metric'], res_info['draw_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def res_plot(epi_res: List, window: int = 250) -> None:\n",
    "    results = np.mean(np.array(epi_res).reshape(-1,window), axis=1)\n",
    "    fig, axes = plt.subplots(figsize=(6, 5))\n",
    "    axes.plot(results)\n",
    "    axes.set_title('Result with avg. window = {} (Totol = {})'.format(window, len(epi_res)))\n",
    "    \n",
    "epi_res = res_info['res']\n",
    "res_plot(epi_res, window=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 TicTacToe with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std\n",
    "import io\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "# imported\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a feedforward neural net with three hidden layers consisting of 128, 256, and 128 neurons\n",
    "- input: 9 poition x 2 possible states\n",
    "- output: nine actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 10000\n",
    "GAMMA = 0.99 # discount factor\n",
    "TARGET_UPDATE = 500 # update interval\n",
    "START_LR = 5e-4\n",
    "EPS_MAX = 0.8\n",
    "EPS_MIN = 0.1\n",
    "N_STAR = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_to_state(\n",
    "    grid: np.array, \n",
    "    switch: bool = False, \n",
    "    vec: bool = True,\n",
    ") -> np.array:\n",
    "    \"\"\"If not switched, 1 and -1 on the grid means X and O, and vice versa\"\"\"\n",
    "    \n",
    "    state = np.zeros((3, 3, 2))\n",
    "    \n",
    "    if not switch:\n",
    "        state[:, :, 0] = (grid==1).astype(float)\n",
    "        state[:, :, 1] = (grid==-1).astype(float)\n",
    "    else:\n",
    "        state[:, :, 0] = (grid==-1).astype(float)\n",
    "        state[:, :, 1] = (grid==1).astype(float)\n",
    "    \n",
    "    if vec:\n",
    "        state = state.reshape(1,-1)\n",
    "    \n",
    "    return torch.tensor(state, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  1.]\n",
      " [-1.  0.  1.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "grid, _, _ = env.reset()\n",
    "grid, _, _ = env.step(2)\n",
    "grid, _, _ = env.step(3)\n",
    "grid, _, _ = env.step(5)\n",
    "print(grid)\n",
    "test_state1 = grid_to_state(grid, vec=False)\n",
    "# print(test_state1[:,:,0], test_state1[:,:,1])\n",
    "test_state2 = grid_to_state(grid, switch=True, vec=False)\n",
    "# print(test_state2[:,:,0], test_state2[:,:,1])\n",
    "assert np.array_equal(test_state1[:,:,0], test_state2[:,:,1])\n",
    "assert np.array_equal(test_state2[:,:,0], test_state1[:,:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[1., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 1.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_state2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replay memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Transition`: a single transition in our environment. It essentially maps (state, action) pairs to their (next_state, reward) result, with the state being the board position (showing -, O, or X)\n",
    "- `ReplayMemory`: a cyclic buffer of bounded size that holds the transitions observed recently, which could be sampled for selecting a random batch of transitions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html#replay-memory\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illegal moves reward (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_UNAV = -1\n",
    "# invalid = env.check_valid(position) == False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input state $s_t$ by a 3 × 3 × 2 tensor\n",
    "- 2 hidden layers each with 128 neurons – with ReLu activation functions.\n",
    "The output layer has 9 neurons (for 9 different actions) with linear activation functions\n",
    "- 9 neurons (for 9 different actions) with linear activation functions\n",
    "\n",
    "PS: do not constraint actions to only available actions. However, whenever the agent takes an unavailable action, we end the game and give the agent a negative reward\n",
    "of value $r_{unav}$ = −1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_inputs: int = 18, n_outputs: int = 9):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_inputs, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, n_outputs)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization.\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    def act(self, state):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(state).max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(\n",
    "    device: torch.device,\n",
    "    optimizer: optim.Optimizer,\n",
    "    criterion: torch.nn,\n",
    "    policy: nn.Module,\n",
    "    target: nn.Module,\n",
    "    memory: ReplayMemory,\n",
    "    batch_size: int,\n",
    "    gamma: float,\n",
    "):\n",
    "    \"\"\"Model optimization step, borrow from the Torch DQN tutorial.\n",
    "    \n",
    "    Arguments:\n",
    "        device {torch.device} -- Device\n",
    "        optimizer {torch.optim.Optimizer} -- Optimizer\n",
    "        criterion {torch.nn} -- Loss\n",
    "        policy {nn.Module} -- Policy net\n",
    "        target {nn.Module} -- Target net\n",
    "        memory {ReplayMemory} -- Replay memory\n",
    "        batch_size {int} -- Number of observations to use per batch step\n",
    "        gamma {float} -- Reward discount factor\n",
    "    \"\"\"\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "        device=device,\n",
    "        dtype=torch.bool,\n",
    "    )\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy\n",
    "    state_action_values = policy(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(batch_size, device=device)\n",
    "    next_state_values[non_final_mask] = target(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = criterion(\n",
    "        state_action_values, \n",
    "        expected_state_action_values.unsqueeze(1)\n",
    "    )\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decreasing_exploration(\n",
    "    n_step: int,\n",
    "    n_star: int = 20000, \n",
    "    e_max: float = 0.8, \n",
    "    e_min: float = 0.1, \n",
    "):\n",
    "    return max(e_min, e_max * (1 - n_step/n_star)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(\n",
    "    eps: float,\n",
    "    policy_net: nn.Module, \n",
    "    state: np.array, \n",
    "    device: torch.device, \n",
    "    nr_action: int = 9,\n",
    ") -> Tuple[torch.tensor, bool]:\n",
    "\n",
    "    if random.random() > eps:\n",
    "        return policy_net.act(state), False\n",
    "    else:\n",
    "        return (\n",
    "            torch.tensor(\n",
    "                [[random.randrange(9)]],\n",
    "                device=device,\n",
    "            ),\n",
    "            True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(\n",
    "    n_episode: int = 20000,\n",
    "    batch_size: int = BATCH_SIZE,\n",
    "    buffer_size: int = BUFFER_SIZE,\n",
    "    gamma: float = GAMMA,\n",
    "    target_update: int = TARGET_UPDATE,\n",
    "    explore: bool = False,\n",
    "    n_star: int = N_STAR, \n",
    "    e_max: float = EPS_MAX, \n",
    "    e_min: float = EPS_MIN, \n",
    "    lr: float = START_LR,\n",
    "    logging_size = 2000,\n",
    "    seed = None,\n",
    ") -> bytes:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    logging.info(\"Beginning training on: {}\".format(device))\n",
    "    \n",
    "    # Network\n",
    "    policy = DQN(n_inputs = 18, n_outputs = 9).to(device)\n",
    "    target = DQN(n_inputs = 18, n_outputs = 9).to(device)\n",
    "    target.load_state_dict(policy.state_dict())\n",
    "    target.eval()\n",
    "\n",
    "    # Adam optimizer\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=lr)\n",
    "    \n",
    "    # TODO: scheduler?\n",
    "\n",
    "    # Huber loss (delte=1 in SmoothL1Loss)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "\n",
    "    # Memory buffer\n",
    "    memory = ReplayMemory(BUFFER_SIZE)\n",
    "\n",
    "    env = TictactoeEnv()\n",
    "    # state = torch.tensor([], dtype=torch.float).to(device)\n",
    "    summary = {\n",
    "        \"total\": 0,\n",
    "        \"ties\": 0,\n",
    "        \"illegals\": 0,\n",
    "        \"wins\": 0,\n",
    "        \"loss\": 0,\n",
    "        \"win_rate\": 0.0,\n",
    "    }\n",
    "    summaries = []\n",
    "    \n",
    "    # Expert\n",
    "    expert = OptimalPlayer(epsilon=.5)\n",
    "\n",
    "    for episode in tqdm(range(n_episode)):\n",
    "        grid, end, _ = env.reset()\n",
    "        \n",
    "        if explore:\n",
    "            eps = decreasing_exploration(episode, n_star, e_max, e_min)\n",
    "        else:\n",
    "            eps = e_max\n",
    "        \n",
    "        switch = episode % 2\n",
    "        if switch:\n",
    "            player1, player2 = 'O', 'X'\n",
    "        else:\n",
    "            player1, player2 = 'X', 'O'\n",
    "        expert.player = player2\n",
    "        \n",
    "        # Take first step by expert\n",
    "        if env.num_step == 0 and switch:\n",
    "            move = expert.act(grid)\n",
    "            grid, end, winner = env.step(move, print_grid=False) \n",
    "            \n",
    "        while not end:\n",
    "            # Select and perform an action from DQN\n",
    "            state = grid_to_state(grid, switch=switch, vec=True).to(device)\n",
    "            action, _ = select_action(eps, policy, state, device)\n",
    "            if env.check_valid(action.item()):\n",
    "                grid, end, winner = env.step(action.item(), print_grid=False)\n",
    "                reward = env.reward(player=player1)\n",
    "            else:\n",
    "                reward = R_UNAV # -1\n",
    "                end = True\n",
    "                summary[\"illegals\"] += 1\n",
    "                winner = player2\n",
    "                # print(episode, \"Illegal moves\")\n",
    "                \n",
    "            if not end:\n",
    "                move = expert.act(grid)\n",
    "                grid, end, winner = env.step(move, print_grid=False) \n",
    "                # Observe new state\n",
    "                next_state = grid_to_state(grid, switch=switch, vec=True).to(device)\n",
    "            else:\n",
    "                next_state = None\n",
    "                \n",
    "            memory.push(state, action, next_state, torch.tensor([reward], device=device))\n",
    "\n",
    "            optimize_model(\n",
    "                device=device,\n",
    "                optimizer=optimizer,\n",
    "                criterion = criterion,\n",
    "                policy=policy,\n",
    "                target=target,\n",
    "                memory=memory,\n",
    "                batch_size=batch_size,\n",
    "                gamma=gamma,\n",
    "            )\n",
    "            \n",
    "        # update summary\n",
    "        summary['total'] = episode + 1\n",
    "        if winner == None:\n",
    "            summary['ties'] += 1\n",
    "        elif winner == player1:\n",
    "            summary['wins'] += 1\n",
    "        else:\n",
    "            summary['loss'] += 1\n",
    "        summary['win_rate'] = summary['wins'] / summary['total']\n",
    "        \n",
    "        if episode % target_update == 0:\n",
    "            target.load_state_dict(policy.state_dict())\n",
    "        if episode % logging_size == 0:\n",
    "            print(summary)\n",
    "#             if summaries != []:\n",
    "#                 old_summary = summaries[-1]\n",
    "#                 delta_summary = {k: summary[k] - old_summary[k] for k in old_summary}\n",
    "#                 logging.info(\"{} : {}\".format(episode, delta_summary))\n",
    "                \n",
    "#             summaries.append(summary)\n",
    "\n",
    "    logging.info(\"Complete\")\n",
    "\n",
    "#     res = io.BytesIO()\n",
    "#     torch.save(policy.state_dict(), res)\n",
    "\n",
    "#     return res.getbuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Beginning training on: cuda\n",
      "  0%|          | 29/20000 [00:00<01:12, 277.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 1, 'ties': 0, 'illegals': 1, 'wins': 0, 'loss': 1, 'win_rate': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2014/20000 [00:26<03:58, 75.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 2001, 'ties': 52, 'illegals': 467, 'wins': 562, 'loss': 1387, 'win_rate': 0.28085957021489255}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4015/20000 [00:52<03:37, 73.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 4001, 'ties': 103, 'illegals': 862, 'wins': 1201, 'loss': 2697, 'win_rate': 0.30017495626093477}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6011/20000 [01:19<03:10, 73.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 6001, 'ties': 138, 'illegals': 1268, 'wins': 1759, 'loss': 4104, 'win_rate': 0.29311781369771706}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8015/20000 [01:47<02:43, 73.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 8001, 'ties': 158, 'illegals': 1640, 'wins': 2270, 'loss': 5573, 'win_rate': 0.2837145356830396}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10007/20000 [02:15<02:17, 72.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 10001, 'ties': 186, 'illegals': 2055, 'wins': 2700, 'loss': 7115, 'win_rate': 0.26997300269973}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12009/20000 [02:44<01:58, 67.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 12001, 'ties': 214, 'illegals': 2475, 'wins': 3175, 'loss': 8612, 'win_rate': 0.2645612865594534}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14012/20000 [03:11<01:21, 73.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 14001, 'ties': 232, 'illegals': 2877, 'wins': 3669, 'loss': 10100, 'win_rate': 0.2620527105206771}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16013/20000 [03:41<00:56, 70.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 16001, 'ties': 263, 'illegals': 3288, 'wins': 4164, 'loss': 11574, 'win_rate': 0.26023373539153805}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18012/20000 [04:09<00:27, 71.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'total': 18001, 'ties': 306, 'illegals': 3678, 'wins': 4674, 'loss': 13021, 'win_rate': 0.25965224154213656}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [04:36<00:00, 72.35it/s]\n",
      "INFO:root:Complete\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "fit(n_episode=20000, e_max=0.2)\n",
    "# res = fit(n_episode=200)\n",
    "# sys.stdout.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNPlayer():\n",
    "    def __init__(self, device: torch.device, player='X'):\n",
    "        self.player = player # 'X' or 'O'\n",
    "        self.device = device\n",
    "        self.model = DQN(n_inputs=18, n_outputs=9).to(self.device)\n",
    "        \n",
    "    def load_model(self, path: str):\n",
    "        model_state_dict = torch.load(path, map_location=self.device)\n",
    "        self.model.load_state_dict(model_state_dict)\n",
    "        self.model.eval()\n",
    "\n",
    "    def act(self, state: torch.tensor):\n",
    "        with torch.no_grad():\n",
    "            p = F.softmax(self.model.forward(state)).cpu().numpy()\n",
    "            valid_moves = (state.cpu().numpy().reshape(3,3,2).argmax(axis=2).reshape(-1) == 0)\n",
    "            p = valid_moves*p\n",
    "            return p.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Learning from experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11 Standard training with fixed  $\\epsilon$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12 Training without the replay buffer and with a batch size of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q13 Training with decreasing $\\epsilon$ given different values of $n*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q14 Visualizing $M_{opt}$ and $M_{rand}$ over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q15 Reporting best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Learning by self-practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q16 Training with different fixed  $\\epsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q17 Training with decreasing  $\\epsilon$ given different values of $n*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q18 Reporting best results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q19 Visualizing Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "248.792px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
